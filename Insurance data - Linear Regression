#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#%%
df=pd.read_csv(r'C:\Users\Lenovo\Documents\Data science\python\Linear regression\datasets_insurance.csv',header=0)
df.head()
#%%
print(df.dtypes)
print(df.shape)
print(df.describe(include='all'))

#%%
df.sex=df.sex.map({'female':0,'male':1})
df.smoker=df.smoker.map({'yes':1,'no':0})
df.region=df.region.map({'southwest':1,'southeast':2,'northwest':3,'northeast':4})
df.head()
#%%
df.bmi=df.bmi.round(2)
df.charges=df.charges.round(2)
df.head()
#%%

#outliers
df.boxplot(column='bmi') 
#%%
df.boxplot(column='charges')

#%%
#linear relationship
sns.pairplot(df,x_vars=['age','bmi'],y_vars='charges',kind='reg')
#%%
X=df[['age','sex','bmi','children','smoker','region']]
Y=df['charges']
#%%
#dependent variable(Y) should follow a aprrox normal distribution
sns.distplot(Y,hist=True)
#%%
Y_log=np.log(Y)
#%%
sns.distplot(Y_log,hist=True)
#%%
#treating skewness in X variables
X.hist(bins=20)
#%%
from scipy.stats import skew
data_num_skew = X.apply(lambda x: skew(x.dropna()))# we are using dropna to remove missing values.IF MISSING VALUES WERE ALREADY TREATED. THEN NOT REQD TO RUN DROPNA. JUST SKEW(X) SHLD  BE  ENUF.
data_num_skewed = data_num_skew[(data_num_skew > .75) | (data_num_skew < -.75)]

print(data_num_skew)
print(data_num_skewed)
#%%
#multicollinearity-correlation
corr_df=X.corr(method='pearson')
print(corr_df)

sns.heatmap(corr_df,vmin=-1,vmax=1,annot=True)
#%%
#vif
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif

vif_df = pd.DataFrame() 
vif_df["features"] = X.columns 
vif_df["VIF Factor"] = [vif(X.values, i) for i in range(X.shape[1])]   
vif_df.round(2)
#%%
#spliting the data
from sklearn.model_selection import train_test_split

X_train,X_test,Y_train,Y_test=train_test_split(X,Y_log,test_size=0.3,random_state=10)

print(Y_train)
#%%
print(X_test.shape)
print(Y_test.shape)
#%%
#auto-correlation and significant variables
new_df1=pd.DataFrame()
new_df1=X_train
new_df1['Premium']=Y_train
#%%
import statsmodels.formula.api as sm
lm_model=sm.ols(formula='Premium ~ age+ sex+ bmi+ children+ smoker+ region',data=new_df1).fit()
print(lm_model.params)
print(lm_model.summary())
'''
6.830637
age 0.03526 sex:-0.0674 bmi:0.0117 children 0.0973 smoker 1.5607 region 0.0533
'''

#%%
X=df[['age','sex','bmi','children','smoker','region']]
Y=df['charges']
#%%
Y_log=np.log(Y)
#%%

from sklearn.preprocessing import StandardScaler

scaler= StandardScaler()
X= scaler.fit_transform(X)
print(X)
#%%
from sklearn.model_selection import train_test_split

X_train, X_test,Y_train,Y_test=train_test_split(X,Y_log,test_size=0.3,random_state=10)
print(Y_train)
#%%
X_test.shape
#%%
from sklearn.linear_model import Ridge
lm=Ridge()
lm.fit(X_train,Y_train)

print(lm.intercept_)
print(lm.coef_)
#%%
'''
9.096852049259512
[ 0.4944642  -0.03501858  0.06814083  0.1196985   0.63246837  0.0472034 ]
'''
#%%
Y_pred=lm.predict(X_test)
#%%
import numpy as np
from sklearn.metrics import r2_score,mean_squared_error
r2 =r2_score(Y_test,Y_pred)
print(r2)

rmse=np.sqrt(mean_squared_error(Y_test,Y_pred))
print(rmse)

adjusted_r2=1-(1-r2)*(len(Y_log)-1)/(len(Y_log)-(X.shape[1]-1))
print(adjusted_r2)
#%%
'''
0.7375188569789608
0.45350948534345537
0.7367312166398129
'''

#%%
new_df=pd.DataFrame(X_test,columns=['age','sex','bmi','children','smoker','region'])
new_df

#%%
Y_actual=Y_test.reset_index()
print(Y_actual)
#%%
Y_actual=Y_actual.drop('index',axis=1)
Y_actual
#%%
new_df['Actual']=Y_actual
new_df.head()
#%%
Y_predicted=pd.Series(Y_pred)
Y_predicted
#%%
new_df['Predicted']=Y_predicted
new_df.head()
#%%
new_df['Deviation']=new_df.Actual-new_df.Predicted
new_df.head()
#%%
new_df.to_excel(r'C:\Users\Lenovo\Documents\Data science\python\Linear regression\Insurance_data.xlsx',header=True,index=True)
#%%
#assumption-1 -  the error pattern is random . it shld not follow  pattern.
import matplotlib.pyplot as plt
plot_lm_1 = plt.figure(1)
plot_lm_1.set_figheight(8)
plot_lm_1.set_figwidth(12)

# fitted values (need a constant term for intercept)
model_fitted_y = lm_model.fittedvalues

plot_lm_1.axes[0] = sns.residplot(model_fitted_y,'Premium', data=new_df1, lowess=True)

plot_lm_1.axes[0].set_title('Residuals vs Fitted')
plot_lm_1.axes[0].set_xlabel('Fitted values')
plot_lm_1.axes[0].set_ylabel('Residuals')
#%%
#%%
#assumption-2- the errors will b normally distributed
res = lm_model.resid
import statsmodels.api as stm
import scipy.stats as stats
fig = stm.qqplot(res, fit=True, line='45')
plt.title('Normal Q-Q')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Standardized Residuals')
plt.show()
#%%
#%%
#ASSUMPTION-3 - ERRORS SHLD  HAVE  HOMOSKADACITY. 
model_norm_residuals = lm_model.get_influence().resid_studentized_internal
# absolute squared normalized residuals
model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))

plot_lm_3 = plt.figure(3)
plot_lm_3.set_figheight(8)
plot_lm_3.set_figwidth(12)
plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)
sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, lowess=True)


plot_lm_3.axes[0].set_title('Scale-Location')
plot_lm_3.axes[0].set_xlabel('Fitted values')
plot_lm_3.axes[0].set_ylabel('$\sqrt{|Standardized Residuals|}$')
